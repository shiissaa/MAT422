{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO359I6SPwPagH90tRwwfVg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiissaa/MAT422/blob/main/3_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.7 Neural Networks**\n",
        "\n",
        "Artificial neural networks is a collection of connected layers of units or nodes\n",
        "to loosely model the neurons in a biological brain. In this section, we illustrate the use of differentiation for training artificial neural networks to minimize cost functions."
      ],
      "metadata": {
        "id": "SJ0wsk1NJ9cX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basics of deep learning: The artificial neuron\n",
        "\n",
        "It takes inputs from the previous layer of neurons (x_1,...,x_n)\n",
        "\n",
        "Weights are associated with each of the inputs, learned during training (w_i,...,w_n)\n",
        "\n",
        "It also has a bias, not associated with an input but learned during training (w_0)\n",
        "\n",
        "The result of a single neuron is derived from a linear combination of the inputs and weights\n",
        "\n",
        "Activation function gets applied\n",
        "\n",
        "Grouped into layers to create a neural network:\n",
        "\n",
        "(1) Input layer\n",
        "(2) Hidden layer\n",
        "(3) Output layer"
      ],
      "metadata": {
        "id": "tXwJg6kSPMHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation functions:\n",
        "\n",
        "Confining linear combination of the inputs to the artificial neuron to a specific range\n",
        "\n",
        "Want it to be differentiable\n",
        "\n",
        "Non-linear functions tend to be preferred (as linear functions do not confine values to a range)\n",
        "\n",
        "Functions that tend to distribute values toward extremes are preferable\n",
        "\n",
        "Choice of activation function often affects the accuracy of the model–especially for the inner layers\n"
      ],
      "metadata": {
        "id": "Xeho6H9nQRtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One such activation function is the softmax function. It converts a vector of numbers (an array of K integer values) into a vector of probabilities, where the probabilities of each value are proportional to the relative scale of each value in the vector. It is thus a function that turns several numbers into quantities that can be perhaps interprted as possibilities. It is often used in the final, output, layer of the neural network, epsecially with classifcation problems. There are other activation functions, like ReLU (Rectified Linear Unit) and its variants, which all have their use.\n",
        "\n",
        "For example, in a feedforward neural network (i.e. each layer accepts input only from the preivous layer, resulting in higher layers not influencing lower layers), normally a ReLU or GELU (Gaussian Error Linear Unit) activation function gets used for its hidden layers, while a linear or softmax activation function will be used for the output layers."
      ],
      "metadata": {
        "id": "NrdQlT7fQYm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backpropagation\n",
        "\n",
        "Back-propagation is the essence of neural network training. It is the practice\n",
        "of fine-tuning the weights of a neural network based on the error rate (i.e.\n",
        "loss) obtained in the previous iteration. Proper tuning of the weights ensures lower error rates, making the model reliable by increasing its generalization. This can be done alongside gradient descent."
      ],
      "metadata": {
        "id": "YMSPfkGoRHRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP, or multi-layer perceptron, trains using backpropagation through the use\n",
        "# of some form of gradient descent, and the gradients are calculated using it\n",
        "# Supports multi-class classification by applying softmax as the output function\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Train on two arrays:\n",
        "# an array of X size (n_samples, n_features), holds the training samples\n",
        "X = [[0., 0.], [1., 1.]]\n",
        "# an array of Y size (n_samples), holds the target values (class labels)\n",
        "y = [0, 1]\n",
        "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
        "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
        "clf.fit(X, y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "oD3QRTxWJ9k3",
        "outputId": "c25e72ac-bace-4108-808b-7f4c92c1332d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\n",
              "              solver='lbfgs')"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\n",
              "              solver=&#x27;lbfgs&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\n",
              "              solver=&#x27;lbfgs&#x27;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After training, the model can predict labels for new samples\n",
        "clf.predict([[2., 2.], [-1., -2.]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1KgawgSS1if",
        "outputId": "83af9380-ecc1-4f0a-f8f1-fdeb73fe538f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}