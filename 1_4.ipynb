{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCln7mcyyGbu8rsbWZXrba",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiissaa/MAT422/blob/main/1_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.4.1 Singular Value Decomposition**\n",
        "\n",
        "Singular value decomposition provides another way of factoring a matrix into singular vectors and singular values. It gives us information that we may want to know.\n",
        "\n",
        "Let $A$ be a $m$ x $n$ matrix. Then $A^TA$ is symmetric and can be orthogonally diagonalized. Let $v_1,...,v_n$ be an orthonormal basis for $R^n$ consisting of eigenvectors of $A^TA$, and let $lambda_n,...,labmbda_n$ be the associated eigenvalues of $A^TA$. With this (and some more) we can see that the eigenvalues of $A$ are all nonnegative."
      ],
      "metadata": {
        "id": "y1c6cCIDo4Jd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from scipy.linalg import svd\n",
        "\n",
        "A = array([[1, 2], [3, 4], [5, 6]])\n",
        "print(A)\n",
        "U, s, VT = svd(A)\n",
        "print(U)\n",
        "print(s)\n",
        "print(VT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMujcV35ralZ",
        "outputId": "42dd5533-d34b-4573-c88c-8b2a115d4001"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2]\n",
            " [3 4]\n",
            " [5 6]]\n",
            "[[-0.2298477   0.88346102  0.40824829]\n",
            " [-0.52474482  0.24078249 -0.81649658]\n",
            " [-0.81964194 -0.40189603  0.40824829]]\n",
            "[9.52551809 0.51430058]\n",
            "[[-0.61962948 -0.78489445]\n",
            " [-0.78489445  0.61962948]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.4.2 Low-Rank Matrix Approximations**\n",
        "\n",
        "The goal of low rank approximations is to have a matrix that one can store with less memory and have be computed faster with the same behavior as the original matrix.\n",
        "\n",
        "To do this, we compute the SVD, keep k left vectors of U, keep k diagonal values of S, and keep k top vectors of V."
      ],
      "metadata": {
        "id": "3CxtkXIIo4Pl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.4.3 Principal Component Analysis**\n",
        "\n",
        "This refers to a dimensionality-reduction method, used to reduce the size and variables of a large data set while aiming to preserve as much info as possible. It's underlying mathematics can be explained with singular value decomposition.\n",
        "\n",
        "First, standardize variables by weighting them equally in terms of range. Then, compute` the covariance matrix, which compares the relationships between the variables, identifying and reducing closely related variables. Third, identitfy the principal components, which means to compute the eigenvectors and eigenvalues of the covariance matrix. Then, generate the feature vector. Finally, project the data on the respective axes."
      ],
      "metadata": {
        "id": "azzUBjiSo4eR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "X = np.random.randint(10, size=(4,3))\n",
        "pca = PCA(n_components = 3, svd_solver = 'full')\n",
        "pca.fit(X)\n",
        "PCA(n_components = 3)\n",
        "print(pca.explained_variance_ratio_)\n",
        "print(pca.singular_values_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AEi4cMmvhEB",
        "outputId": "74d1e158-b1ad-477d-b7d8-6ebcc889703d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5929196  0.38406524 0.02301515]\n",
            "[9.06197357 7.29335562 1.78538477]\n"
          ]
        }
      ]
    }
  ]
}