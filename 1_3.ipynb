{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOon74K9oVqTxTq2tEUR0B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiissaa/MAT422/blob/main/1_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.3.1 QR Decomposition**\n",
        "\n",
        "QR decomposition is useful for solving the linear least squares problem. It uses the Gram-Schmidt algorithm to obtain an orthonormal basis $span(a_1,...,a_m)$ from a linearly indepndent set off $span(a_1,...,a_m)$. It takes the form of $A=QR$ where $Q$ is the product of the Gram-Schmidt process, and R is a special type of matrix."
      ],
      "metadata": {
        "id": "uLYQ4c6RUnkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import qr\n",
        "\n",
        "# Decomposes matrix [[0, 2], [2, 3]]\n",
        "a = np.array([[0, 2], [2, 3]])\n",
        "q, r = qr(a)\n",
        "print('Q:', q)\n",
        "print('R:', r)\n",
        "b = np.dot(q, r)\n",
        "print('QR:', b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvisePQkWxyC",
        "outputId": "08e5307d-43f9-4985-d12c-2ca2cceb317d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: [[ 0. -1.]\n",
            " [-1.  0.]]\n",
            "R: [[-2. -3.]\n",
            " [ 0. -2.]]\n",
            "QR: [[0. 2.]\n",
            " [2. 3.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can also be used to get the eigenvalues of matrix $A$."
      ],
      "metadata": {
        "id": "2x-EZr-cXnJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = [1, 5, 10, 20]\n",
        "# 20 iterations\n",
        "for i in range(20):\n",
        "  q, r = qr(a)\n",
        "  a = np.dot(r, q)\n",
        "  if i+1 in p:\n",
        "    print(f'Iteration {i+1}:')\n",
        "    print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1Cc4blTXrPJ",
        "outputId": "f79a180e-666d-4a6e-abcc-ece81adcac90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1:\n",
            "[[ 4.00000000e+00  2.32830633e-09]\n",
            " [ 2.32830644e-09 -1.00000000e+00]]\n",
            "Iteration 5:\n",
            "[[ 4.00000000e+00  9.09484250e-12]\n",
            " [ 9.09494702e-12 -1.00000000e+00]]\n",
            "Iteration 10:\n",
            "[[ 4.00000000e+00  8.98629997e-15]\n",
            " [ 8.88178420e-15 -1.00000000e+00]]\n",
            "Iteration 20:\n",
            "[[ 4.00000000e+00  1.04524239e-16]\n",
            " [ 8.47032947e-21 -1.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.3.2 Least-squares Problems**\n",
        "\n",
        "The least-squares problem provides us a way of solving a system when $A$ is not a square matrix, meaning we have an over-determined case where $n>m$, and are interested in using $Ax$ to approximate $b$."
      ],
      "metadata": {
        "id": "d9FaEjNwUnnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In python, there are multiple ways of going about implementing Least-squares Problems\n",
        "# This uses the direct inverse method\n",
        "x = np.linspace(0, 1, 101)\n",
        "y = 1 + x + x * np.random.random(len(x))\n",
        "E = np.vstack([x, np.ones(len(x))]).T\n",
        "y = y[:, np.newaxis]\n",
        "c = np.dot((np.dot(np.linalg.inv(np.dot(E.T,E)),E.T)),y)\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmSdhaQGainU",
        "outputId": "ef47e2b9-842c-41f8-91bb-b7a0fc4908ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.66164321]\n",
            " [0.97073807]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.3.3 Linear Regression**\n",
        "\n",
        "Given inputs, we seek an affine function to fit the data. The common approach involves finding coefficients that minimize certain criteria. The minimization problem can be formulated in matrix form, which can be transfformed into a least-squares problem."
      ],
      "metadata": {
        "id": "2JpsXGJvUnuo"
      }
    }
  ]
}